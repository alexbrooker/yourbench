# ==========================================================
# CODEWRITING_GUIDELINES COMPLIANT
# QG Tendency Analysis - Enhanced Visualization
# ==========================================================
"""
Analyze the tendency of a model to generate questions (both single-shot and multi-hop)
with enhanced visualization capabilities.

This script:
1) Loads single-shot and multi-hop question datasets from the pipeline outputs.
2) Computes:
   - Counts of questions generated by each model
   - Distribution of difficulty ratings (per model)
   - Distribution of question and answer lengths (per model)
   - Correlations between difficulty and length metrics
3) Produces elegant, compact plots at ~3x3 inches, 300 DPI, following the
   project's Graph Generation Guidelines.
4) Creates both individual visualizations and a comprehensive dashboard.
5) Saves all plots to 'plots/qg_tendency/' directory.

Usage:
    yourbench analyze qg_tendency

Configuration:
    This module tries to load:
      config["analysis"]["qg_tendency"] (optional)
    It also reads from config["hf_configuration"]["global_dataset_name"] to
    identify the base dataset. Subset names default to:
      single_shot_subset = "single_shot_questions_deduplicated"
      multi_hop_subset   = "multi_hop_questions_deduplicated"
    If those subsets or the corresponding datasets are missing, warnings are logged.

Output Plots:
    1) question_count_comparison.png - Counts of questions by model
    2) difficulty_distribution.png - Violin plots of difficulty distributions
    3) question_answer_length.png - Relationship between Q and A lengths
    4) length_difficulty_correlation.png - Correlation analysis
    5) comprehensive_dashboard.png - Integrated dashboard with multiple metrics
    6) single_hop_ratio.png - Ratio of single-hop questions generated by model
    7) multi_hop_ratio.png - Ratio of multi-hop questions generated by model
"""

import os
import math
import numpy as np
from typing import List, Dict, Any, Optional, Tuple, Union

from loguru import logger
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
from matplotlib.patches import Patch
from matplotlib.gridspec import GridSpec
from matplotlib.figure import Figure
from matplotlib.axes import Axes
import seaborn as sns

from datasets import Dataset

# YourBench utilities
from yourbench.utils.dataset_engine import smart_load_dataset
from yourbench.utils.dataset_engine import smart_get_source_dataset_name
from yourbench.utils.dataset_engine import smart_get_source_subset

# Model display name mapping
model_mapping = {
    "Qwen/Qwen2.5-72B-Instruct": "Qwen2.5 72B",
    "deepseek-chat": "DeepSeek V3 671B",
    "Qwen/Qwen2.5-VL-72B-Instruct": "Qwen2.5 VL 72B",
    "deepseek-reasoner": "DeepSeek R1 671B",
    "Qwen/QwQ-32B" : "QwQ 32B",
    "meta-llama/Llama-3.3-70B-Instruct" : "Llama 3.3 70B",
    "allenai/OLMo-2-0325-32B-Instruct" : "OLMo 2 32B",
    "mistralai/Mistral-Small-3.1-24B-Instruct-2503" : "Mistral 3.1 24B",
    "gpt-4o" : "GPT 4o",
    "gpt-4o-mini-Sumuk" : "GPT 4o Mini",
    "claude-3-7-sonnet-20250219" : "Claude 3.7 Sonnet",
    "claude-3-5-haiku-20241022" : "Claude 3.5 Haiku",
}

# === Helper Functions for Data Processing ===

def _safe_make_dir(dir_path: str) -> None:
    """
    Create a directory if it doesn't exist, logging any issues gracefully.
    """
    try:
        os.makedirs(dir_path, exist_ok=True)
    except Exception as e:
        logger.error(f"Could not create directory '{dir_path}': {e}")

def _count_questions_by_model(dataset: Dataset) -> Dict[str, int]:
    """
    Count questions generated by each model in the dataset.
    
    Args:
        dataset: The dataset containing questions with model information
        
    Returns:
        Dictionary mapping model names to question counts
    """
    if len(dataset) == 0:
        return {}
        
    # Get column that contains model info
    model_col = None
    for col in ["model", "model_name", "generating_model"]:
        if col in dataset.column_names:
            model_col = col
            break
    
    if not model_col:
        logger.warning("Could not find model column in dataset")
        return {}
    
    # Count questions by model
    counts: Dict[str, int] = {}
    
    for item in dataset:
        model_name = item[model_col]
        
        # Use display name if available
        display_name = model_mapping.get(model_name, model_name)
        
        if display_name in counts:
            counts[display_name] += 1
        else:
            counts[display_name] = 1
    
    return counts

def _create_question_ratio_plot(
    model_counts: Dict[str, int], 
    plot_title: str, 
    output_path: str
) -> None:
    """
    Create a bar chart showing the raw counts of questions generated by each model,
    with ratio annotations above each bar.
    
    Args:
        model_counts: Dictionary mapping model names to question counts
        plot_title: Title for the plot
        output_path: Path to save the output image
        
    The model with the lowest count is labeled as "1x" and used as the baseline.
    Other models show their ratio compared to this baseline as annotations.
    Models are sorted in ascending order of question count.
    The y-axis shows the raw question counts.
    """
    if not model_counts:
        logger.warning(f"No model counts to plot for {plot_title}")
        return
    
    # Sort models by count
    sorted_models = sorted(model_counts.items(), key=lambda x: x[1])
    models = [m[0] for m in sorted_models]
    counts = [m[1] for m in sorted_models]
    
    # Calculate ratios for annotations
    min_count = counts[0]
    ratios = [count / min_count for count in counts]
    
    # Create plot
    plt.figure(figsize=(3, 3), dpi=300)
    
    # Use a subtle, professional color palette
    palette = sns.color_palette("mako", len(models))
    
    # Create the bar chart with raw counts
    bars = plt.bar(range(len(models)), counts, color=palette)
    
    # Add ratio labels on top of each bar
    for i, (bar, ratio) in enumerate(zip(bars, ratios)):
        if i == 0:
            label = "1x"
        else:
            label = f"{ratio:.1f}x"
        plt.text(
            bar.get_x() + bar.get_width() / 2,
            bar.get_height() + 0.05 * max(counts),
            label,
            ha='center',
            va='bottom',
            fontsize=8,
        )
    
    # Customize the plot
    plt.title(plot_title, fontsize=10)
    plt.ylabel("Number of Questions", fontsize=8)
    plt.xticks(range(len(models)), [m for m in models], rotation=45, ha='right', fontsize=7)
    plt.yticks(fontsize=7)
    
    # Format y-axis labels with k suffix for thousands
    plt.gca().yaxis.set_major_formatter(
        plt.FuncFormatter(lambda x, pos: f'{int(x/1000)}k' if x >= 1000 else str(int(x)))
    )
    
    # Add padding to y-axis (20% extra space above highest bar)
    max_count = max(counts)
    plt.ylim(0, max_count * 1.2)
    
    plt.tight_layout()
    
    # Save plot
    try:
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        logger.info(f"Saved question ratio plot to {output_path}")
    except Exception as e:
        logger.error(f"Failed to save question ratio plot: {e}")
    
    plt.close()

# === Main Analysis Entry Point ===

def run(*args: str) -> None:
    """
    Run the QG Tendency Analysis.

    Steps:
      1. Load single-shot Q dataset (default subset: single_shot_questions_deduplicated).
      2. Load multi-hop Q dataset (default subset: multi_hop_questions_deduplicated).
      3. Compute question counts, difficulty distributions, question lengths,
         answer lengths by model.
      4. Generate enhanced visualizations with clear narrative elements.
      5. Create a comprehensive dashboard integrating multiple metrics.
      6. Save all plots to 'plots/qg_tendency/' directory.
    """
    logger.info("Running question generation tendency analysis with enhanced visualization...")

    # Attempt to read config from global scope:
    # We'll do a minimal approach: if no config is found, we log a warning and skip.
    from yourbench.analysis import run_analysis  # not used, but for consistency
    from yourbench.config_cache import get_last_config
    from yourbench.utils.loading_engine import load_config

    config_path = get_last_config()
    if not config_path:
        logger.warning("No cached config found. Using default logic. Some subsets may be unavailable.")
        # We'll proceed with defaults only
        config = {}
    else:
        try:
            config = load_config(config_path)
        except Exception as e:
            logger.error(f"Failed to load config from {config_path}: {e}")
            config = {}

    # Default subsets
    single_shot_subset = "single_shot_questions"
    multi_hop_subset   = "multi_hop_questions"

    # Attempt to load from config["analysis"]["qg_tendency"] if present
    qgt_cfg = config.get("analysis", {}).get("qg_tendency", {})
    single_shot_subset = qgt_cfg.get("single_shot_subset", single_shot_subset)
    multi_hop_subset   = qgt_cfg.get("multi_hop_subset", multi_hop_subset)

    # We'll need the base dataset name from HF config (or fallback)
    dataset_name = config.get("hf_configuration", {}).get("global_dataset_name", "yourbench_dataset")

    # === 1) Load single-shot dataset ===
    try:
        ss_dataset = smart_load_dataset(dataset_name, config, dataset_subset=single_shot_subset)
        logger.info(f"Loaded single-shot Q dataset: {single_shot_subset} with {len(ss_dataset)} rows.")
    except Exception as e:
        logger.warning(f"Could not load single-shot dataset subset='{single_shot_subset}': {e}")
        ss_dataset = Dataset.from_dict({})  # empty

    # === 2) Load multi-hop dataset ===
    try:
        mh_dataset = smart_load_dataset(dataset_name, config, dataset_subset=multi_hop_subset)
        logger.info(f"Loaded multi-hop Q dataset: {multi_hop_subset} with {len(mh_dataset)} rows.")
    except Exception as e:
        logger.warning(f"Could not load multi-hop dataset subset='{multi_hop_subset}': {e}")
        mh_dataset = Dataset.from_dict({})  # empty

    # Create a folder to hold the plots
    output_dir = os.path.join("plots", "qg_tendency")
    _safe_make_dir(output_dir)

    # === Compute metrics ===
    # Count questions by model
    ss_model_counts = _count_questions_by_model(ss_dataset)
    mh_model_counts = _count_questions_by_model(mh_dataset)
    
    # Generate ratio plots
    if ss_model_counts:
        _create_question_ratio_plot(
            ss_model_counts,
            "Single-hop Question Generation Tendency", 
            os.path.join(output_dir, "single_hop_ratio.png")
        )
    
    if mh_model_counts:
        _create_question_ratio_plot(
            mh_model_counts,
            "Multi-hop Question Generation Tendency", 
            os.path.join(output_dir, "multi_hop_ratio.png")
        )
    
    logger.success("Enhanced QG Tendency Analysis is complete. Plots saved to '{}'".format(output_dir))