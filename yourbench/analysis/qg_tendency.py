# ==========================================================
# CODEWRITING_GUIDELINES COMPLIANT
# QG Tendency Analysis - Enhanced Visualization
# ==========================================================
"""
Analyze the tendency of a model to generate questions (both single-shot and multi-hop)
with enhanced visualization capabilities.

This script:
1) Loads single-shot and multi-hop question datasets from the pipeline outputs.
2) Computes:
   - Counts of questions generated by each model
   - Distribution of difficulty ratings (per model)
   - Distribution of question and answer lengths (per model)
   - Correlations between difficulty and length metrics
3) Produces elegant, compact plots at ~3x3 inches, 300 DPI, following the
   project's Graph Generation Guidelines.
4) Creates both individual visualizations and a comprehensive dashboard.
5) Saves all plots to 'plots/qg_tendency/' directory.

Usage:
    yourbench analyze qg_tendency

Configuration:
    This module tries to load:
      config["analysis"]["qg_tendency"] (optional)
    It also reads from config["hf_configuration"]["global_dataset_name"] to
    identify the base dataset. Subset names default to:
      single_shot_subset = "single_shot_questions_deduplicated"
      multi_hop_subset   = "multi_hop_questions_deduplicated"
    If those subsets or the corresponding datasets are missing, warnings are logged.

Output Plots:
    1) question_count_comparison.png - Counts of questions by model
    2) difficulty_distribution.png - Violin plots of difficulty distributions
    3) question_answer_length.png - Relationship between Q and A lengths
    4) length_difficulty_correlation.png - Correlation analysis
    5) comprehensive_dashboard.png - Integrated dashboard with multiple metrics
"""

import os
import math
import numpy as np
from typing import List, Dict, Any, Optional, Tuple, Union

from loguru import logger
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
from matplotlib.patches import Patch
from matplotlib.gridspec import GridSpec
from matplotlib.figure import Figure
from matplotlib.axes import Axes

from datasets import Dataset

# YourBench utilities
from yourbench.utils.dataset_engine import smart_load_dataset
from yourbench.utils.dataset_engine import smart_get_source_dataset_name
from yourbench.utils.dataset_engine import smart_get_source_subset

# Model display name mapping
model_mapping = {
    "Qwen/Qwen2.5-72B-Instruct": "Qwen2.5 72B",
    "deepseek-chat": "DeepSeek v3",
    "Qwen/Qwen2.5-VL-72B-Instruct": "Qwen2.5 VL 72B",
    "deepseek-reasoner": "DeepSeek R1 671B",
    "Qwen/QwQ-32B" : "QwQ 32B",
    "meta-llama/Llama-3.3-70B-Instruct" : "Llama 3.3 70B",
    "allenai/OLMo-2-0325-32B-Instruct" : "OLMo 2 32B",
    "mistralai/Mistral-Small-3.1-24B-Instruct-2503" : "Mistral 3.1 24B"
}

# === Helper Functions for Data Processing ===

def _safe_make_dir(dir_path: str) -> None:
    """
    Create a directory if it doesn't exist, logging any issues gracefully.
    """
    try:
        os.makedirs(dir_path, exist_ok=True)
    except Exception as e:
        logger.error(f"Could not create directory '{dir_path}': {e}")


def _compute_counts_by_model(dataset: Dataset, label: str) -> Dict[str, int]:
    """
    Count total questions by 'generating_model' within a dataset.

    Args:
        dataset (Dataset): The dataset (single_shot or multi_hop).
        label (str): Short label to log or denote single_shot/multi_hop.

    Returns:
        Dict[str, int]: model_name -> number_of_questions
    """
    if not len(dataset):
        logger.warning(f"{label}: dataset is empty. Returning empty counts.")
        return {}

    model_counts = {}
    # Expect a 'generating_model' column
    for row in dataset:
        model_name = row.get("generating_model", "unknown_model")
        model_counts[model_name] = model_counts.get(model_name, 0) + 1

    logger.info(f"{label}: found {len(dataset)} total questions across {len(model_counts)} models.")
    return model_counts


def _compute_difficulty_by_model(dataset: Dataset, label: str) -> Dict[str, List[float]]:
    """
    Gather the 'estimated_difficulty' values by model.

    Returns:
        Dict[str, List[float]]: model_name -> list_of_difficulty_values
    """
    difficulties = {}
    for row in dataset:
        model_name = row.get("generating_model", "unknown_model")
        diff_val = row.get("estimated_difficulty", None)
        if diff_val is None:
            continue
        difficulties.setdefault(model_name, []).append(diff_val)
    logger.info(f"{label}: Collected difficulty info for {len(difficulties)} models.")
    return difficulties


def _compute_text_lengths(dataset: Dataset, question_col: str, answer_col: str, label: str
                         ) -> Tuple[Dict[str, List[int]], Dict[str, List[int]]]:
    """
    Return dictionaries mapping model_name -> list_of_question_lengths (in tokens),
    and model_name -> list_of_answer_lengths (in tokens).

    Args:
        dataset (Dataset): question dataset with question/answer columns
        question_col (str): name of the question column, typically "question"
        answer_col (str): name of the answer column, typically "self_answer"
        label (str): short label for logging

    Returns:
        (Dict[str, List[int]], Dict[str, List[int]])
         (model_name -> question_lengths, model_name -> answer_lengths)
    """
    q_lengths = {}
    a_lengths = {}
    for row in dataset:
        model_name = row.get("generating_model", "unknown_model")
        question_text = row.get(question_col, "")
        answer_text = row.get(answer_col, "")
        # simple token approach
        question_len = len(question_text.split())
        answer_len = len(answer_text.split())
        q_lengths.setdefault(model_name, []).append(question_len)
        a_lengths.setdefault(model_name, []).append(answer_len)

    logger.info(f"{label}: Collected question/answer length data for {len(q_lengths)} models.")
    return q_lengths, a_lengths


def _compute_aggregate_stats(data_dict: Dict[str, List[float]], label: str) -> Dict[str, Dict[str, float]]:
    """
    Compute aggregate statistics (mean, median, std) for each model's data.
    
    Args:
        data_dict: Dictionary mapping model names to lists of values
        label: Label for logging
        
    Returns:
        Dictionary with aggregate statistics
    """
    stats = {}
    for model, values in data_dict.items():
        if not values:
            continue
        stats[model] = {
            'mean': np.mean(values),
            'median': np.median(values),
            'std': np.std(values)
        }
    logger.info(f"{label}: Computed aggregate stats for {len(stats)} models")
    return stats


def _compute_correlations(metric1: Dict[str, List[float]], 
                         metric2: Dict[str, List[float]], 
                         label: str) -> Dict[str, float]:
    """
    Compute correlation coefficients between two metrics for each model.
    
    Args:
        metric1: First metric (e.g., difficulties)
        metric2: Second metric (e.g., lengths)
        label: Label for logging
        
    Returns:
        Dictionary with correlation coefficients by model
    """
    common_models = set(metric1.keys()) & set(metric2.keys())
    correlations = {}
    
    for model in common_models:
        values1 = metric1[model]
        values2 = metric2[model]
        
        # Need matching length arrays
        min_len = min(len(values1), len(values2))
        if min_len < 10:  # Need reasonable sample size
            continue
            
        values1 = values1[:min_len]
        values2 = values2[:min_len]
        
        # Calculate correlation
        try:
            corr = np.corrcoef(values1, values2)[0, 1]
            correlations[model] = corr
        except:
            logger.warning(f"Could not compute correlation for {model}")
    
    logger.info(f"{label}: Computed correlations for {len(correlations)} models")
    return correlations


# === Enhanced Visualization Functions ===

def plot_question_counts(ss_counts: Dict[str, int], 
                         mh_counts: Dict[str, int], 
                         output_path: str) -> None:
    """
    Create a bar chart comparing single-shot and multi-hop question counts by model.
    
    Args:
        ss_counts: Dictionary of single-shot question counts by model
        mh_counts: Dictionary of multi-hop question counts by model
        output_path: Output file path
    """
    # Get all unique models
    all_models = sorted(set(list(ss_counts.keys()) + list(mh_counts.keys())))
    display_names = [model_mapping.get(m, m) for m in all_models]
    
    # Prepare data with 0 for missing values
    ss_values = [ss_counts.get(model, 0) for model in all_models]
    mh_values = [mh_counts.get(model, 0) for model in all_models]
    
    # Sort by total question count
    total_counts = [ss + mh for ss, mh in zip(ss_values, mh_values)]
    sorted_indices = np.argsort(total_counts)[::-1]  # Descending order
    
    display_names = [display_names[i] for i in sorted_indices]
    ss_values = [ss_values[i] for i in sorted_indices]
    mh_values = [mh_values[i] for i in sorted_indices]
    
    # Set up figure
    fig, ax = plt.subplots(figsize=(3, 3), dpi=300)
    
    # Bar positions
    x = np.arange(len(display_names))
    width = 0.4
    
    # Create bars with soft gradient colors
    ax.bar(x - width/2, ss_values, width, label='Single-Shot', 
           color='#4a90e2', alpha=0.8, edgecolor='white', linewidth=0.5)
    ax.bar(x + width/2, mh_values, width, label='Multi-Hop', 
           color='#ff6b6b', alpha=0.8, edgecolor='white', linewidth=0.5)
    
    # Add counts as text on top of bars (only for significant values)
    for i, v in enumerate(ss_values):
        if v > max(ss_values) * 0.05:  # Only label if at least 5% of max
            ax.text(i - width/2, v + max(ss_values)*0.02, str(v), 
                   ha='center', va='bottom', fontsize=6, rotation=0)
            
    for i, v in enumerate(mh_values):
        if v > max(mh_values) * 0.05:  # Only label if at least 5% of max
            ax.text(i + width/2, v + max(mh_values)*0.02, str(v), 
                   ha='center', va='bottom', fontsize=6, rotation=0)
    
    # Styling
    ax.set_title('Question Generation by Model Type', fontsize=9, pad=10)
    ax.set_xlabel('Model', fontsize=8)
    ax.set_ylabel('Number of Questions', fontsize=8)
    ax.set_xticks(x)
    ax.set_xticklabels(display_names, rotation=45, ha="right", fontsize=7)
    ax.tick_params(axis='y', labelsize=7)
    
    # Subtle grid
    ax.yaxis.grid(True, linestyle='--', alpha=0.3, zorder=0)
    ax.set_axisbelow(True)
    
    # Legend with refined styling
    ax.legend(fontsize=7, framealpha=0.7, edgecolor='none', loc='upper right')
    
    plt.tight_layout()
    plt.savefig(output_path)
    plt.close()
    logger.info(f"Saved question count comparison plot to '{output_path}'")


def plot_difficulty_distribution(ss_difficulties: Dict[str, List[float]], 
                                mh_difficulties: Dict[str, List[float]], 
                                output_path: str) -> None:
    """
    Create violin plots showing difficulty distribution comparison between
    single-shot and multi-hop questions across models.
    
    Args:
        ss_difficulties: Dictionary of difficulty ratings for single-shot questions
        mh_difficulties: Dictionary of difficulty ratings for multi-hop questions
        output_path: Output file path
    """
    # Find common models
    common_models = set(ss_difficulties.keys()) & set(mh_difficulties.keys())
    if not common_models:
        logger.warning("No common models found for difficulty distribution plot")
        return
    
    # Calculate median difficulties for sorting
    model_medians = {}
    for model in common_models:
        combined = ss_difficulties[model] + mh_difficulties[model]
        model_medians[model] = np.median(combined)
    
    sorted_models = sorted(model_medians.items(), key=lambda x: x[1])
    model_names = [model for model, _ in sorted_models]
    display_names = [model_mapping.get(m, m) for m in model_names]
    
    # Figure setup
    fig, ax = plt.subplots(figsize=(3, 3), dpi=300)
    
    # Data for plotting
    ss_data = [ss_difficulties[model] for model in model_names]
    mh_data = [mh_difficulties[model] for model in model_names]
    
    # Positions
    positions = np.arange(1, len(model_names) + 1)
    
    # Create violins with gradient coloring
    violin_parts1 = ax.violinplot(ss_data, positions=positions-0.2, 
                               widths=0.3, showmeans=False, showmedians=True)
    violin_parts2 = ax.violinplot(mh_data, positions=positions+0.2, 
                               widths=0.3, showmeans=False, showmedians=True)
    
    # Style violins
    for pc in violin_parts1['bodies']:
        pc.set_facecolor('#4a90e2')
        pc.set_edgecolor('none')
        pc.set_alpha(0.7)
    
    for pc in violin_parts2['bodies']:
        pc.set_facecolor('#ff6b6b')
        pc.set_edgecolor('none')
        pc.set_alpha(0.7)
    
    # Style median lines
    for partname in ['cmedians']:
        violin_parts1[partname].set_edgecolor('black')
        violin_parts1[partname].set_linewidth(1)
        violin_parts2[partname].set_edgecolor('black')
        violin_parts2[partname].set_linewidth(1)
    
    # Add average difficulty annotation for clarity
    for i, model in enumerate(model_names):
        ss_avg = np.mean(ss_difficulties[model])
        mh_avg = np.mean(mh_difficulties[model])
        
        if len(model_names) <= 3:  # Only add text annotations if few models
            ax.text(positions[i]-0.2, 0.2, f"{ss_avg:.1f}", ha='center', 
                   va='bottom', fontsize=6, color='#1a4e8c')
            ax.text(positions[i]+0.2, 0.2, f"{mh_avg:.1f}", ha='center', 
                   va='bottom', fontsize=6, color='#a02a2a')
    
    # Styling
    ax.set_title('Question Difficulty Distribution', fontsize=9, pad=10)
    ax.set_ylabel('Difficulty Rating (1-10)', fontsize=8)
    ax.set_xlabel('Model', fontsize=8)
    
    # Set tick positions and labels
    ax.set_xticks(positions)
    ax.set_xticklabels(display_names, rotation=45, ha='right', fontsize=7)
    ax.tick_params(axis='y', labelsize=7)
    
    # Add legend
    legend_elements = [
        Patch(facecolor='#4a90e2', alpha=0.7, label='Single-Shot'),
        Patch(facecolor='#ff6b6b', alpha=0.7, label='Multi-Hop')
    ]
    ax.legend(handles=legend_elements, fontsize=7, loc='upper right', framealpha=0.7)
    
    # Add subtle grid for readability
    ax.yaxis.grid(True, linestyle='--', alpha=0.3)
    ax.set_axisbelow(True)
    
    plt.tight_layout()
    plt.savefig(output_path)
    plt.close()
    logger.info(f"Saved difficulty distribution plot to '{output_path}'")


def plot_length_relationship(q_lengths: Dict[str, List[int]], 
                            a_lengths: Dict[str, List[int]], 
                            title: str, 
                            output_path: str) -> None:
    """
    Create a scatter plot showing Q/A length relationship with trend lines.
    
    Args:
        q_lengths: Dictionary of question lengths by model
        a_lengths: Dictionary of answer lengths by model
        title: Plot title
        output_path: Output file path
    """
    # Find common models
    common_models = set(q_lengths.keys()) & set(a_lengths.keys())
    
    if not common_models:
        logger.warning(f"No common models found for '{title}' plot")
        return
    
    # Set up figure
    fig, ax = plt.subplots(figsize=(3, 3), dpi=300)
    
    # Use a color gradient for points
    cmap = plt.cm.viridis
    
    # For each model, gather data and plot
    for i, model in enumerate(sorted(common_models)[:5]):  # Limit to 5 models
        q_lens = q_lengths[model]
        a_lens = a_lengths[model]
        
        # Need matching lengths
        min_len = min(len(q_lens), len(a_lens))
        q_lens = q_lens[:min_len]
        a_lens = a_lens[:min_len]
        
        # Skip if not enough data
        if min_len < 10:
            continue
            
        # Sample to avoid overcrowding (at most 200 points per model)
        sample_rate = max(1, min_len // 200)
        q_lens = q_lens[::sample_rate]
        a_lens = a_lens[::sample_rate]
        
        # Calculate ratio
        model_display = model_mapping.get(model, model)
        ratio = np.mean(a_lens) / np.mean(q_lens)
        
        # Plot with semi-transparency and size based on frequency
        scatter = ax.scatter(q_lens, a_lens, s=15, alpha=0.4, 
                           label=f"{model_display} (ratio: {ratio:.1f})",
                           color=cmap(i/5))
    
    # Add line showing 1:1 ratio
    max_val = max(ax.get_xlim()[1], ax.get_ylim()[1])
    ax.plot([0, max_val], [0, max_val], 'k--', alpha=0.3, linewidth=0.8)
    
    # Add text explaining the ratio
    ax.text(0.05, 0.95, "Points above line:\nA > Q length", transform=ax.transAxes,
           fontsize=6, verticalalignment='top', bbox=dict(boxstyle='round,pad=0.3', 
                                                         facecolor='white', alpha=0.7))
    
    # Styling
    ax.set_title(title, fontsize=9, pad=10)
    ax.set_xlabel('Question Length (tokens)', fontsize=8)
    ax.set_ylabel('Answer Length (tokens)', fontsize=8)
    ax.tick_params(axis='both', labelsize=7)
    
    # Add grid for readability
    ax.grid(True, linestyle='--', alpha=0.3)
    
    # Optimize legend placement
    ax.legend(fontsize=6, loc='lower right', framealpha=0.7)
    
    plt.tight_layout()
    plt.savefig(output_path)
    plt.close()
    logger.info(f"Saved length relationship plot to '{output_path}'")


def plot_length_difficulty_correlation(difficulties: Dict[str, List[float]], 
                                      lengths: Dict[str, List[int]], 
                                      title: str, 
                                      output_path: str) -> None:
    """
    Show correlation between question/answer length and difficulty.
    
    Args:
        difficulties: Dictionary of difficulty ratings by model
        lengths: Dictionary of text lengths by model
        title: Plot title
        output_path: Output file path
    """
    # Find common models
    common_models = set(difficulties.keys()) & set(lengths.keys())
    
    if not common_models:
        logger.warning(f"No common models found for '{title}' plot")
        return
    
    # Set up figure
    fig, ax = plt.subplots(figsize=(3, 3), dpi=300)
    
    # For storing correlation values
    correlations = {}
    
    # Color palette
    colors = plt.cm.tab10(np.linspace(0, 1, len(common_models)))
    
    for i, model in enumerate(sorted(common_models)):
        model_diff = difficulties[model]
        model_len = lengths[model]
        
        # Need matching lengths
        min_len = min(len(model_diff), len(model_len))
        if min_len < 10:  # Skip if not enough data
            continue
            
        model_diff = model_diff[:min_len]
        model_len = model_len[:min_len]
            
        # Calculate correlation
        corr = np.corrcoef(model_diff, model_len)[0, 1]
        correlations[model] = corr
        
        # Sample to avoid overcrowding (at most 150 points per model)
        sample_rate = max(1, min_len // 150)
        
        # Plot with color indicating the model
        model_display = model_mapping.get(model, model)
        ax.scatter(model_len[::sample_rate], model_diff[::sample_rate], 
                  s=15, alpha=0.5, color=colors[i % len(colors)],
                  label=f"{model_display[:10]} (r={corr:.2f})")
        
        # Add trendline if correlation is significant
        if abs(corr) > 0.1 and min_len > 20:
            # Calculate trendline
            z = np.polyfit(model_len, model_diff, 1)
            p = np.poly1d(z)
            
            # Get x range for this model
            x_min, x_max = min(model_len), max(model_len)
            x_line = np.linspace(x_min, x_max, 100)
            
            # Plot trendline
            ax.plot(x_line, p(x_line), '-', color=colors[i % len(colors)], 
                   alpha=0.7, linewidth=1)
    
    # Add annotation showing overall relationship trend
    if correlations:
        avg_corr = np.mean(list(correlations.values()))
        rel_text = "positive" if avg_corr > 0.1 else "negative" if avg_corr < -0.1 else "weak"
        
        ax.text(0.05, 0.95, f"Overall: {rel_text} correlation\n(avg r={avg_corr:.2f})", 
               transform=ax.transAxes, fontsize=6, verticalalignment='top',
               bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.7))
    
    # Styling
    ax.set_title(title, fontsize=9, pad=10)
    ax.set_xlabel('Length (tokens)', fontsize=8)
    ax.set_ylabel('Difficulty Rating (1-10)', fontsize=8)
    ax.tick_params(axis='both', labelsize=7)
    
    # Add grid for readability
    ax.grid(True, linestyle='--', alpha=0.3)
    
    # Optimize legend placement
    ax.legend(fontsize=6, loc='best', framealpha=0.7)
    
    plt.tight_layout()
    plt.savefig(output_path)
    plt.close()
    logger.info(f"Saved length-difficulty correlation plot to '{output_path}'")


def plot_answer_ratio_by_model(q_lengths: Dict[str, List[int]], 
                              a_lengths: Dict[str, List[int]], 
                              output_path: str) -> None:
    """
    Create a bar chart showing average answer-to-question length ratio by model.
    
    Args:
        q_lengths: Dictionary of question lengths by model
        a_lengths: Dictionary of answer lengths by model
        output_path: Output file path
    """
    # Find common models
    common_models = set(q_lengths.keys()) & set(a_lengths.keys())
    
    if not common_models:
        logger.warning("No common models found for answer ratio plot")
        return
    
    # Calculate ratios
    ratios = {}
    for model in common_models:
        q_lens = q_lengths[model]
        a_lens = a_lengths[model]
        
        # Get matching arrays
        min_len = min(len(q_lens), len(a_lens))
        if min_len < 10:  # Skip if not enough data
            continue
            
        q_lens = q_lens[:min_len]
        a_lens = a_lens[:min_len]
        
        # Calculate average ratio
        avg_ratio = np.mean(a_lens) / np.mean(q_lens)
        ratios[model] = avg_ratio
    
    # Sort and prepare for plotting
    sorted_ratios = sorted(ratios.items(), key=lambda x: x[1], reverse=True)
    model_names = [model for model, _ in sorted_ratios]
    ratio_values = [ratio for _, ratio in sorted_ratios]
    display_names = [model_mapping.get(m, m) for m in model_names]
    
    # Figure setup
    fig, ax = plt.subplots(figsize=(3, 3), dpi=300)
    
    # Create horizontal bars with gradient coloring
    cmap = plt.cm.YlGnBu
    color_vals = np.linspace(0.3, 0.9, len(model_names))
    bars = ax.barh(display_names, ratio_values, 
                  color=[cmap(val) for val in color_vals], alpha=0.8)
    
    # Add ratio values as text on the bars
    for i, v in enumerate(ratio_values):
        ax.text(v + 0.1, i, f"{v:.1f}x", va='center', fontsize=7)
    
    # Add reference line and annotation
    ax.axvline(x=1, color='gray', linestyle='--', alpha=0.7, linewidth=1)
    ax.text(1, -0.5, "1:1 ratio", ha='center', va='top', fontsize=6, color='gray')
    
    # Styling
    ax.set_title('Answer-to-Question Length Ratio by Model', fontsize=9, pad=10)
    ax.set_xlabel('Average A-to-Q Ratio', fontsize=8)
    ax.set_xlim(0, max(ratio_values) * 1.2)
    ax.tick_params(axis='both', labelsize=7)
    
    plt.tight_layout()
    plt.savefig(output_path)
    plt.close()
    logger.info(f"Saved answer-to-question ratio plot to '{output_path}'")


def create_comprehensive_dashboard(ss_counts: Dict[str, int], 
                                  mh_counts: Dict[str, int],
                                  ss_difficulties: Dict[str, List[float]], 
                                  mh_difficulties: Dict[str, List[float]],
                                  ss_q_lengths: Dict[str, List[int]], 
                                  ss_a_lengths: Dict[str, List[int]],
                                  mh_q_lengths: Dict[str, List[int]], 
                                  mh_a_lengths: Dict[str, List[int]],
                                  output_path: str) -> None:
    """
    Create a comprehensive dashboard integrating multiple QG tendency metrics.
    
    Args:
        ss_counts: Single-shot question counts by model
        mh_counts: Multi-hop question counts by model
        ss_difficulties: Single-shot difficulty ratings by model
        mh_difficulties: Multi-hop difficulty ratings by model
        ss_q_lengths: Single-shot question lengths by model
        ss_a_lengths: Single-shot answer lengths by model
        mh_q_lengths: Multi-hop question lengths by model
        mh_a_lengths: Multi-hop answer lengths by model
        output_path: Output file path
    """
    # Set up figure with subplots
    fig = plt.figure(figsize=(9, 7), dpi=300)
    gs = fig.add_gridspec(6, 8, hspace=0.6, wspace=0.5)
    
    # === Find all unique models for consistent colors ===
    all_models = set()
    for d in [ss_counts, mh_counts, ss_difficulties, mh_difficulties,
              ss_q_lengths, ss_a_lengths, mh_q_lengths, mh_a_lengths]:
        all_models.update(d.keys())
    all_models = sorted(all_models)
    
    # Create color mapping for consistent model colors
    color_map = {}
    cmap = plt.cm.tab10
    for i, model in enumerate(all_models):
        color_map[model] = cmap(i % 10)
    
    # === 1. Question Counts Comparison (Top Left) ===
    ax1 = fig.add_subplot(gs[0:2, 0:4])
    
    # Get all unique models for counts
    count_models = sorted(set(list(ss_counts.keys()) + list(mh_counts.keys())))
    display_names = [model_mapping.get(m, m) for m in count_models]
    
    # Prepare data with 0 for missing values
    ss_values = [ss_counts.get(model, 0) for model in count_models]
    mh_values = [mh_counts.get(model, 0) for model in count_models]
    
    # Sort by total question count
    total_counts = [ss + mh for ss, mh in zip(ss_values, mh_values)]
    sorted_indices = np.argsort(total_counts)[::-1]  # Descending order
    
    display_names = [display_names[i] for i in sorted_indices]
    ss_values = [ss_values[i] for i in sorted_indices]
    mh_values = [mh_values[i] for i in sorted_indices]
    
    # Bar positions
    x = np.arange(len(display_names))
    width = 0.4
    
    # Create bars with soft gradient colors
    ax1.bar(x - width/2, ss_values, width, label='Single-Shot', 
           color='#4a90e2', alpha=0.8, edgecolor='white', linewidth=0.5)
    ax1.bar(x + width/2, mh_values, width, label='Multi-Hop', 
           color='#ff6b6b', alpha=0.8, edgecolor='white', linewidth=0.5)
    
    # Add counts as text for top bars
    for i, v in enumerate(ss_values):
        if v > max(ss_values) * 0.1:  # Only label if significant
            ax1.text(i - width/2, v + max(ss_values)*0.02, str(v), 
                    ha='center', va='bottom', fontsize=6, rotation=0)
            
    for i, v in enumerate(mh_values):
        if v > max(mh_values) * 0.1:  # Only label if significant
            ax1.text(i + width/2, v + max(mh_values)*0.02, str(v), 
                    ha='center', va='bottom', fontsize=6, rotation=0)
    
    ax1.set_title('Question Count by Model Type', fontsize=10, pad=10)
    ax1.set_xlabel('Model', fontsize=8)
    ax1.set_ylabel('Number of Questions', fontsize=8)
    ax1.set_xticks(x)
    ax1.set_xticklabels(display_names, rotation=45, ha="right", fontsize=7)
    ax1.tick_params(axis='y', labelsize=7)
    ax1.yaxis.grid(True, linestyle='--', alpha=0.3, zorder=0)
    ax1.set_axisbelow(True)
    ax1.legend(fontsize=7, framealpha=0.7, edgecolor='none', loc='upper right')
    
    # === 2. Difficulty Distribution (Top Right) ===
    ax2 = fig.add_subplot(gs[0:2, 4:8])
    
    # Find common models
    diff_models = set(ss_difficulties.keys()) & set(mh_difficulties.keys())
    
    # Calculate median difficulties for sorting
    model_medians = {}
    for model in diff_models:
        combined = ss_difficulties[model] + mh_difficulties[model]
        model_medians[model] = np.median(combined)
    
    sorted_diff_models = sorted(model_medians.items(), key=lambda x: x[1])
    diff_model_names = [model for model, _ in sorted_diff_models]
    diff_display_names = [model_mapping.get(m, m) for m in diff_model_names]
    
    # Data for plotting
    ss_diff_data = [ss_difficulties[model] for model in diff_model_names]
    mh_diff_data = [mh_difficulties[model] for model in diff_model_names]
    
    # Positions
    positions = np.arange(1, len(diff_model_names) + 1)
    
    # Create violins with gradient coloring
    violin_parts1 = ax2.violinplot(ss_diff_data, positions=positions-0.2, 
                                 widths=0.3, showmeans=False, showmedians=True)
    violin_parts2 = ax2.violinplot(mh_diff_data, positions=positions+0.2, 
                                 widths=0.3, showmeans=False, showmedians=True)
    
    # Style violins
    for pc in violin_parts1['bodies']:
        pc.set_facecolor('#4a90e2')
        pc.set_edgecolor('none')
        pc.set_alpha(0.7)
    
    for pc in violin_parts2['bodies']:
        pc.set_facecolor('#ff6b6b')
        pc.set_edgecolor('none')
        pc.set_alpha(0.7)
    
    # Style median lines
    for partname in ['cmedians']:
        violin_parts1[partname].set_edgecolor('black')
        violin_parts1[partname].set_linewidth(1)
        violin_parts2[partname].set_edgecolor('black')
        violin_parts2[partname].set_linewidth(1)
    
    # Add average difficulty annotation
    for i, model in enumerate(diff_model_names):
        ss_avg = np.mean(ss_difficulties[model])
        mh_avg = np.mean(mh_difficulties[model])
        
        if len(diff_model_names) <= 3:  # Only add text annotations if few models
            ax2.text(positions[i]-0.2, 0.2, f"{ss_avg:.1f}", ha='center', 
                    va='bottom', fontsize=6, color='#1a4e8c')
            ax2.text(positions[i]+0.2, 0.2, f"{mh_avg:.1f}", ha='center', 
                    va='bottom', fontsize=6, color='#a02a2a')
    
    ax2.set_title('Question Difficulty Distribution', fontsize=10, pad=10)
    ax2.set_ylabel('Difficulty Rating (1-10)', fontsize=8)
    ax2.set_xlabel('Model', fontsize=8)
    ax2.set_xticks(positions)
    ax2.set_xticklabels(diff_display_names, rotation=45, ha='right', fontsize=7)
    ax2.tick_params(axis='y', labelsize=7)
    
    legend_elements = [
        Patch(facecolor='#4a90e2', alpha=0.7, label='Single-Shot'),
        Patch(facecolor='#ff6b6b', alpha=0.7, label='Multi-Hop')
    ]
    ax2.legend(handles=legend_elements, fontsize=7, loc='upper right', framealpha=0.7)
    ax2.yaxis.grid(True, linestyle='--', alpha=0.3)
    ax2.set_axisbelow(True)
    
    # === 3. Question Length vs Answer Length (Single-Shot) (Middle Left) ===
    ax3 = fig.add_subplot(gs[2:4, 0:4])
    
    # Find common models
    common_ss_models = set(ss_q_lengths.keys()) & set(ss_a_lengths.keys())
    
    # For each model, gather data and plot
    for i, model in enumerate(sorted(common_ss_models)[:5]):  # Limit to 5 models
        q_lens = ss_q_lengths[model]
        a_lens = ss_a_lengths[model]
        
        # Need matching lengths
        min_len = min(len(q_lens), len(a_lens))
        if min_len < 10:
            continue
            
        q_lens = q_lens[:min_len]
        a_lens = a_lens[:min_len]
            
        # Sample to avoid overcrowding (at most 200 points per model)
        sample_rate = max(1, min_len // 200)
        q_lens = q_lens[::sample_rate]
        a_lens = a_lens[::sample_rate]
        
        # Calculate ratio
        model_display = model_mapping.get(model, model)
        ratio = np.mean(a_lens) / np.mean(q_lens)
        
        # Plot with consistent model colors
        ax3.scatter(q_lens, a_lens, s=15, alpha=0.4, 
                   label=f"{model_display} (ratio: {ratio:.1f})",
                   color=color_map[model])
    
    # Add line showing 1:1 ratio
    max_val = max(ax3.get_xlim()[1], ax3.get_ylim()[1])
    ax3.plot([0, max_val], [0, max_val], 'k--', alpha=0.3, linewidth=0.8)
    
    ax3.set_title('Single-Shot: Q/A Length Relationship', fontsize=10, pad=10)
    ax3.set_xlabel('Question Length (tokens)', fontsize=8)
    ax3.set_ylabel('Answer Length (tokens)', fontsize=8)
    ax3.tick_params(axis='both', labelsize=7)
    ax3.grid(True, linestyle='--', alpha=0.3)
    ax3.legend(fontsize=6, loc='upper left', framealpha=0.7)
    
    # === 4. Question Length vs Answer Length (Multi-Hop) (Middle Right) ===
    ax4 = fig.add_subplot(gs[2:4, 4:8])
    
    # Find common models
    common_mh_models = set(mh_q_lengths.keys()) & set(mh_a_lengths.keys())
    
    # For each model, gather data and plot
    for i, model in enumerate(sorted(common_mh_models)[:5]):  # Limit to 5 models
        q_lens = mh_q_lengths[model]
        a_lens = mh_a_lengths[model]
        
        # Need matching lengths
        min_len = min(len(q_lens), len(a_lens))
        if min_len < 10:
            continue
            
        q_lens = q_lens[:min_len]
        a_lens = a_lens[:min_len]
            
        # Sample to avoid overcrowding (at most 200 points per model)
        sample_rate = max(1, min_len // 200)
        q_lens = q_lens[::sample_rate]
        a_lens = a_lens[::sample_rate]
        
        # Calculate ratio
        model_display = model_mapping.get(model, model)
        ratio = np.mean(a_lens) / np.mean(q_lens)
        
        # Plot with consistent model colors
        ax4.scatter(q_lens, a_lens, s=15, alpha=0.4, 
                   label=f"{model_display} (ratio: {ratio:.1f})",
                   color=color_map[model])
    
    # Add line showing 1:1 ratio
    max_val = max(ax4.get_xlim()[1], ax4.get_ylim()[1])
    ax4.plot([0, max_val], [0, max_val], 'k--', alpha=0.3, linewidth=0.8)
    
    ax4.set_title('Multi-Hop: Q/A Length Relationship', fontsize=10, pad=10)
    ax4.set_xlabel('Question Length (tokens)', fontsize=8)
    ax4.set_ylabel('Answer Length (tokens)', fontsize=8)
    ax4.tick_params(axis='both', labelsize=7)
    ax4.grid(True, linestyle='--', alpha=0.3)
    ax4.legend(fontsize=6, loc='upper left', framealpha=0.7)
    
    # === 5. Length-Difficulty Correlation (Single-Shot) (Bottom Left) ===
    ax5 = fig.add_subplot(gs[4:6, 0:4])
    
    # Find common models
    common_ss_diff_len = set(ss_difficulties.keys()) & set(ss_q_lengths.keys())
    
    # For storing correlation values
    correlations_ss = {}
    
    for i, model in enumerate(sorted(common_ss_diff_len)):
        model_diff = ss_difficulties[model]
        model_len = ss_q_lengths[model]
        
        # Need matching lengths
        min_len = min(len(model_diff), len(model_len))
        if min_len < 10:  # Skip if not enough data
            continue
            
        model_diff = model_diff[:min_len]
        model_len = model_len[:min_len]
            
        # Calculate correlation
        corr = np.corrcoef(model_diff, model_len)[0, 1]
        correlations_ss[model] = corr
        
        # Sample to avoid overcrowding (at most 150 points per model)
        sample_rate = max(1, min_len // 150)
        
        # Plot with consistent model colors
        model_display = model_mapping.get(model, model)
        ax5.scatter(model_len[::sample_rate], model_diff[::sample_rate], 
                   s=15, alpha=0.5, color=color_map[model],
                   label=f"{model_display[:10]} (r={corr:.2f})")
        
        # Add trendline if correlation is significant
        if abs(corr) > 0.1 and min_len > 20:
            # Calculate trendline
            z = np.polyfit(model_len, model_diff, 1)
            p = np.poly1d(z)
            
            # Get x range for this model
            x_min, x_max = min(model_len), max(model_len)
            x_line = np.linspace(x_min, x_max, 100)
            
            # Plot trendline
            ax5.plot(x_line, p(x_line), '-', color=color_map[model], 
                    alpha=0.7, linewidth=1)
    
    # Add annotation showing overall relationship trend
    if correlations_ss:
        avg_corr = np.mean(list(correlations_ss.values()))
        rel_text = "positive" if avg_corr > 0.1 else "negative" if avg_corr < -0.1 else "weak"
        
        ax5.text(0.05, 0.95, f"Overall: {rel_text} correlation\n(avg r={avg_corr:.2f})", 
                transform=ax5.transAxes, fontsize=6, verticalalignment='top',
                bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.7))
    
    ax5.set_title('Single-Shot: Length vs Difficulty', fontsize=10, pad=10)
    ax5.set_xlabel('Question Length (tokens)', fontsize=8)
    ax5.set_ylabel('Difficulty Rating (1-10)', fontsize=8)
    ax5.tick_params(axis='both', labelsize=7)
    ax5.grid(True, linestyle='--', alpha=0.3)
    ax5.legend(fontsize=6, loc='best', framealpha=0.7)
    
    # === 6. Length-Difficulty Correlation (Multi-Hop) (Bottom Right) ===
    ax6 = fig.add_subplot(gs[4:6, 4:8])
    
    # Find common models
    common_mh_diff_len = set(mh_difficulties.keys()) & set(mh_q_lengths.keys())
    
    # For storing correlation values
    correlations_mh = {}
    
    for i, model in enumerate(sorted(common_mh_diff_len)):
        model_diff = mh_difficulties[model]
        model_len = mh_q_lengths[model]
        
        # Need matching lengths
        min_len = min(len(model_diff), len(model_len))
        if min_len < 10:  # Skip if not enough data
            continue
            
        model_diff = model_diff[:min_len]
        model_len = model_len[:min_len]
            
        # Calculate correlation
        corr = np.corrcoef(model_diff, model_len)[0, 1]
        correlations_mh[model] = corr
        
        # Sample to avoid overcrowding (at most 150 points per model)
        sample_rate = max(1, min_len // 150)
        
        # Plot with consistent model colors
        model_display = model_mapping.get(model, model)
        ax6.scatter(model_len[::sample_rate], model_diff[::sample_rate], 
                   s=15, alpha=0.5, color=color_map[model],
                   label=f"{model_display[:10]} (r={corr:.2f})")
        
        # Add trendline if correlation is significant
        if abs(corr) > 0.1 and min_len > 20:
            # Calculate trendline
            z = np.polyfit(model_len, model_diff, 1)
            p = np.poly1d(z)
            
            # Get x range for this model
            x_min, x_max = min(model_len), max(model_len)
            x_line = np.linspace(x_min, x_max, 100)
            
            # Plot trendline
            ax6.plot(x_line, p(x_line), '-', color=color_map[model], 
                    alpha=0.7, linewidth=1)
    
    # Add annotation showing overall relationship trend
    if correlations_mh:
        avg_corr = np.mean(list(correlations_mh.values()))
        rel_text = "positive" if avg_corr > 0.1 else "negative" if avg_corr < -0.1 else "weak"
        
        ax6.text(0.05, 0.95, f"Overall: {rel_text} correlation\n(avg r={avg_corr:.2f})", 
                transform=ax6.transAxes, fontsize=6, verticalalignment='top',
                bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.7))
    
    ax6.set_title('Multi-Hop: Length vs Difficulty', fontsize=10, pad=10)
    ax6.set_xlabel('Question Length (tokens)', fontsize=8)
    ax6.set_ylabel('Difficulty Rating (1-10)', fontsize=8)
    ax6.tick_params(axis='both', labelsize=7)
    ax6.grid(True, linestyle='--', alpha=0.3)
    ax6.legend(fontsize=6, loc='best', framealpha=0.7)
    
    # Main title for the dashboard
    fig.suptitle('Question Generation Tendency Analysis Dashboard', 
                fontsize=12, y=0.98, fontweight='bold')
    
    plt.tight_layout(rect=[0, 0, 1, 0.96])
    plt.savefig(output_path)
    plt.close()
    logger.info(f"Saved comprehensive dashboard to '{output_path}'")


# === Main Analysis Entry Point ===

def run(*args: str) -> None:
    """
    Run the QG Tendency Analysis.

    Steps:
      1. Load single-shot Q dataset (default subset: single_shot_questions_deduplicated).
      2. Load multi-hop Q dataset (default subset: multi_hop_questions_deduplicated).
      3. Compute question counts, difficulty distributions, question lengths,
         answer lengths by model.
      4. Generate enhanced visualizations with clear narrative elements.
      5. Create a comprehensive dashboard integrating multiple metrics.
      6. Save all plots to 'plots/qg_tendency/' directory.
    """
    logger.info("Running question generation tendency analysis with enhanced visualization...")

    # Attempt to read config from global scope:
    # We'll do a minimal approach: if no config is found, we log a warning and skip.
    from yourbench.analysis import run_analysis  # not used, but for consistency
    from yourbench.config_cache import get_last_config
    from yourbench.utils.loading_engine import load_config

    config_path = get_last_config()
    if not config_path:
        logger.warning("No cached config found. Using default logic. Some subsets may be unavailable.")
        # We'll proceed with defaults only
        config = {}
    else:
        try:
            config = load_config(config_path)
        except Exception as e:
            logger.error(f"Failed to load config from {config_path}: {e}")
            config = {}

    # Default subsets
    single_shot_subset = "single_shot_questions"
    multi_hop_subset   = "multi_hop_questions"

    # Attempt to load from config["analysis"]["qg_tendency"] if present
    qgt_cfg = config.get("analysis", {}).get("qg_tendency", {})
    single_shot_subset = qgt_cfg.get("single_shot_subset", single_shot_subset)
    multi_hop_subset   = qgt_cfg.get("multi_hop_subset", multi_hop_subset)

    # We'll need the base dataset name from HF config (or fallback)
    dataset_name = config.get("hf_configuration", {}).get("global_dataset_name", "yourbench_dataset")

    # === 1) Load single-shot dataset ===
    try:
        ss_dataset = smart_load_dataset(dataset_name, config, dataset_subset=single_shot_subset)
        logger.info(f"Loaded single-shot Q dataset: {single_shot_subset} with {len(ss_dataset)} rows.")
    except Exception as e:
        logger.warning(f"Could not load single-shot dataset subset='{single_shot_subset}': {e}")
        ss_dataset = Dataset.from_dict({})  # empty

    # === 2) Load multi-hop dataset ===
    try:
        mh_dataset = smart_load_dataset(dataset_name, config, dataset_subset=multi_hop_subset)
        logger.info(f"Loaded multi-hop Q dataset: {multi_hop_subset} with {len(mh_dataset)} rows.")
    except Exception as e:
        logger.warning(f"Could not load multi-hop dataset subset='{multi_hop_subset}': {e}")
        mh_dataset = Dataset.from_dict({})  # empty

    # Create a folder to hold the plots
    output_dir = os.path.join("plots", "qg_tendency")
    _safe_make_dir(output_dir)

    # === Compute metrics ===
    
    # (A) Question counts by model
    ss_counts = _compute_counts_by_model(ss_dataset, "SingleShot")
    mh_counts = _compute_counts_by_model(mh_dataset, "MultiHop")
    
    # (B) Distribution of estimated difficulty
    ss_difficulties = _compute_difficulty_by_model(ss_dataset, "SingleShot")
    mh_difficulties = _compute_difficulty_by_model(mh_dataset, "MultiHop")
    
    # (C) Distribution of question & answer lengths
    ss_q_lengths, ss_a_lengths = _compute_text_lengths(
        ss_dataset, question_col="question", answer_col="self_answer", label="SingleShot"
    )
    mh_q_lengths, mh_a_lengths = _compute_text_lengths(
        mh_dataset, question_col="question", answer_col="self_answer", label="MultiHop"
    )
    
    # === Generate Enhanced Visualizations ===
    
    # 1. Question Count Comparison
    plot_question_counts(
        ss_counts, 
        mh_counts, 
        output_path=os.path.join(output_dir, "question_count_comparison.png")
    )
    
    # 2. Difficulty Distribution
    plot_difficulty_distribution(
        ss_difficulties, 
        mh_difficulties, 
        output_path=os.path.join(output_dir, "difficulty_distribution.png")
    )
    
    # 3. Question-Answer Length Relationship
    plot_length_relationship(
        ss_q_lengths, 
        ss_a_lengths, 
        title="Single-Shot: Question vs Answer Length",
        output_path=os.path.join(output_dir, "question_answer_length_singleshot.png")
    )
    
    plot_length_relationship(
        mh_q_lengths, 
        mh_a_lengths, 
        title="Multi-Hop: Question vs Answer Length",
        output_path=os.path.join(output_dir, "question_answer_length_multihop.png")
    )
    
    # 4. Length-Difficulty Correlation
    plot_length_difficulty_correlation(
        ss_difficulties, 
        ss_q_lengths, 
        title="Single-Shot: Question Length vs Difficulty",
        output_path=os.path.join(output_dir, "length_difficulty_correlation_singleshot.png")
    )
    
    plot_length_difficulty_correlation(
        mh_difficulties, 
        mh_q_lengths, 
        title="Multi-Hop: Question Length vs Difficulty",
        output_path=os.path.join(output_dir, "length_difficulty_correlation_multihop.png")
    )
    
    # 5. Answer-to-Question Ratio
    plot_answer_ratio_by_model(
        ss_q_lengths, 
        ss_a_lengths, 
        output_path=os.path.join(output_dir, "answer_question_ratio_singleshot.png")
    )
    
    plot_answer_ratio_by_model(
        mh_q_lengths, 
        mh_a_lengths, 
        output_path=os.path.join(output_dir, "answer_question_ratio_multihop.png")
    )
    
    # 6. Comprehensive Dashboard
    create_comprehensive_dashboard(
        ss_counts, 
        mh_counts,
        ss_difficulties,
        mh_difficulties,
        ss_q_lengths,
        ss_a_lengths,
        mh_q_lengths,
        mh_a_lengths,
        output_path=os.path.join(output_dir, "comprehensive_dashboard.png")
    )

    logger.success("Enhanced QG Tendency Analysis is complete. Plots saved to '{}'".format(output_dir))